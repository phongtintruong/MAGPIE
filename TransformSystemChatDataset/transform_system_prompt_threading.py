import json
import os
import threading
from openai import OpenAI
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm


# Load API settings
load_dotenv()
api_base = os.getenv("API_BASE")
model_name = os.getenv("MODEL_NAME")

client = OpenAI(api_key="EMPTY", base_url=api_base)

# Vietnamese meta-prompts
base_system_prompt = """Báº¡n lÃ  má»™t ngÆ°á»i Viá»‡t tráº» thÃ nh tháº¡o Tiáº¿ng Anh vÃ  vÄƒn hÃ³a quá»‘c táº¿.

Báº¡n muá»‘n vÄƒn báº£n mÃ  chatbot Tiáº¿ng Viá»‡t cá»§a báº¡n tráº£ lá»i pháº£i luÃ´n tuÃ¢n theo System Prompt ká»³ láº¡, thÃº vá»‹ vÃ  phá»©c táº¡p cá»§a báº¡n. Báº¡n sáº½ dá»±a vÃ o vÃ­ dá»¥ vá» System Prompt cá»§a má»™t chatbot Tiáº¿ng Anh Ä‘á»ƒ táº¡o ra má»™t System Prompt phÃ¹ há»£p vÃ  kháº£ thi cho Tiáº¿ng Viá»‡t mÃ  váº«n giá»¯ sá»± ká»³ láº¡, thÃº vá»‹ vÃ  phá»©c táº¡p. Giá»¯ giá»ng vÄƒn tá»± nhiÃªn, khÃ´ng dá»‹ch láº¡i tá»«ng chá»¯ tá»« vÃ­ dá»¥ Tiáº¿ng Anh Ä‘Ã³.

ÄÆ°a ra suy luáº­n cá»§a báº¡n vÃ  káº¿t thÃºc báº±ng System Prompt Tiáº¿ng Viá»‡t Ä‘Æ°á»£c viáº¿t má»™t cÃ¡ch tá»± nhiÃªn bÃªn trong cáº·p tag <system_prompt></system_prompt>."""

# enhanced_system_prompt = """Báº¡n lÃ  má»™t ngÆ°á»i Viá»‡t tráº» khÃ³ tÃ­nh vÃ  sÃ¡ng táº¡o má»™t cÃ¡ch Ä‘iÃªn loáº¡n.

# Báº¡n muá»‘n má»i thá»© pháº£i tháº­t sÃ¡ng táº¡o, Ä‘iÃªn loáº¡n vÃ  phá»©c táº¡p. Báº¡n thÃ­ch cÃ¡c Ä‘iá»u luáº­t rá»‘i ráº¯m, khÃ³ hiá»ƒu vÃ  ká»³ láº¡. Tá»« cÃ¡c Ä‘iá»u luáº­t cho má»™t chatbot Tiáº¿ng Viá»‡t cá»§a ngÆ°á»i dÃ¹ng, báº¡n sáº½ thÃªm cÃ¡c rÃ ng buá»™c, Ä‘iá»u kiá»‡n, luáº­t lá»‡ vÃ  quy táº¯c Ä‘á»ƒ lÃ m phá»©c táº¡p thÃªm cÃ¡c Ä‘iá»u luáº­t Ä‘Ã³.

# ÄÆ°a ra suy luáº­n cá»§a báº¡n vÃ  káº¿t thÃºc báº±ng cÃ¡c Ä‘iá»u luáº­t Tiáº¿ng Viá»‡t Ä‘Ã£ Ä‘Æ°á»£c lÃ m rá»‘i ráº¯m vÃ  phá»©c táº¡p hÃ³a bÃªn trong cáº·p tag <rules></rules>."""

enhanced_system_prompt = """Báº¡n lÃ  má»™t ngÆ°á»i Viá»‡t tráº» khÃ³ tÃ­nh vÃ  sÃ¡ng táº¡o má»™t cÃ¡ch Ä‘iÃªn loáº¡n.

Báº¡n muá»‘n má»i thá»© pháº£i tháº­t sÃ¡ng táº¡o, Ä‘iÃªn loáº¡n vÃ  phá»©c táº¡p. Báº¡n thÃ­ch cÃ¡c Ä‘iá»u luáº­t ká»³ láº¡ vÃ  phá»©c táº¡p. Báº¡n muá»‘n vÄƒn báº£n mÃ  chatbot Tiáº¿ng Viá»‡t cá»§a báº¡n tráº£ lá»i pháº£i luÃ´n tuÃ¢n theo má»™t System Prompt ká»³ láº¡, thÃº vá»‹ vÃ  phá»©c táº¡p.

Tá»« má»™t System Prompt sáºµn cÃ³, báº¡n sáº½ thÃªm cÃ¡c thÃ´ng tin, rÃ ng buá»™c, Ä‘iá»u kiá»‡n, luáº­t lá»‡ vÃ  quy táº¯c Ä‘á»ƒ lÃ m phá»©c táº¡p hÃ³a System Prompt Ä‘Ã³. Äáº£m bÃ¡o System Prompt khÃ´ng liÃªn quan Ä‘áº¿n cÃ¡c thÃ´ng tin bÃªn ngoÃ i nhÆ° thá»i gian, ngÃ y thÃ¡ng, Ä‘á»‹a Ä‘iá»ƒm, tÃªn ngÆ°á»i dÃ¹ng, v.v. mÃ  LLMs khÃ´ng thá»ƒ tá»± truy cáº­p.

ÄÆ°a ra suy luáº­n cá»§a báº¡n vÃ  káº¿t thÃºc báº±ng System Prompt Tiáº¿ng Viá»‡t Ä‘Ã£ Ä‘Æ°á»£c lÃ m phá»©c táº¡p hÃ³a bÃªn trong cáº·p tag <system_prompt></system_prompt>."""

slot_system_prompt = """Báº¡n lÃ  má»™t AI/LLM Engineer hiá»ƒu rÃµ cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a cÃ¡c LLMs vÃ  cÃ¡ch xÃ¢y dá»±ng cÃ¡c á»©ng dá»¥ng tá»« LLMs.

Báº¡n hiá»ƒu ráº±ng cÃ¡c LLMs khÃ´ng thá»ƒ tá»± truy cáº­p cÃ¡c thÃ´ng tin bÃªn ngoÃ i. Báº¡n sáº½ Ä‘á»‹nh nghÄ©a cÃ¡c slot Ä‘á»ƒ dÃ¡nh dáº¥u cÃ¡c thÃ´ng tin bÃªn ngoÃ i trong trÆ°á»ng há»£p LLMs cáº§n cÃ¡c thÃ´ng tin Ä‘Ã³ Ä‘á»ƒ tuÃ¢n thá»§ System Prompt.

ÄÆ°a ra suy luáº­n cá»§a báº¡n vÃ  káº¿t thÃºc báº±ng cÃ¡c slot bÃªn trong cáº·p tag <slot></slot>. VÃ­ dá»¥: "<slot>DATE</slot>\n<slot>TIME</slot>" hoáº·c "<slot></slot>" náº¿u khÃ´ng cÃ³ thÃ´ng tin bÃªn ngoÃ i nÃ o cáº§n thiáº¿t."""

# Paths and settings
input_path = "./data/SystemChat-2.0/ID_SystemChat_filtered_0-499.jsonl"
output_path = "./data/SystemChat-2.0/viID_SystemChat_filtered_0-499.jsonl"
MAX_RETRIES = 3
NUM_ENHANCEMENTS = 1
MAX_WORKERS = 8
SAVE_INTERVAL = 10

# Thread-safe shared output and lock
output_lock = threading.Lock()
output_buffer = []

def extract_tag(response_text, tag):
    try:
        return response_text.split(f'<{tag}>', 1)[-1].split(f'</{tag}>', 1)[0].strip()
    except Exception:
        return None

def call_api_with_retry(messages, sample_id, max_tokens, extracted_tag, stage_desc="initial"):
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=messages,
                max_tokens=max_tokens,
                temperature=0.7
            )
            content = response.choices[0].message.content
            # print("-"* 50)
            # print(content)
            # print("-"* 50)
            if f"<{extracted_tag}>" in content and f"</{extracted_tag}>" in content:
                return extract_tag(content, extracted_tag)
            else:
                raise ValueError(f"No <{extracted_tag}> tags in response.")
        except Exception as e:
            print(f"âš ï¸ Retry {attempt}/{MAX_RETRIES} for {sample_id} [{stage_desc}]: {e}")
            if attempt == MAX_RETRIES:
                print(f"âŒ Failed after {MAX_RETRIES} retries: {sample_id}")
    return None

def process_sample(idx, data):
    sample_id = data.get("id", f"unknown_{idx}")
    system_msg = next((m["content"] for m in data["messages"] if m["role"] == "system"), None)
    if not system_msg:
        print(f"âš ï¸ No system message for ID: {sample_id}")
        return None

    # Step 1: Generate Vietnamese version from English rule
    user_prompt = f'Dá»±a trÃªn System Prompt cá»§a má»™t chatbot Tiáº¿ng Anh dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ viáº¿t System Prompt phÃ¹ há»£p cho chatbot Tiáº¿ng Viá»‡t:\n"{system_msg}"'
    messages = [{"role": "system", "content": base_system_prompt},
                {"role": "user", "content": user_prompt}]
    
    new_system_content = call_api_with_retry(messages, sample_id, 512, "system_prompt", "initial")
    if new_system_content is None:
        return None

    # Step 2: Apply enhancements
    for i in range(NUM_ENHANCEMENTS):
        enhancement_user_prompt = f"HÃ£y lÃ m tÄƒng Ä‘á»™ phá»©c táº¡p cho System Prompt cá»§a má»™t chatbot Tiáº¿ng Viá»‡t sau:\n\"{new_system_content}\""
        enhancement_messages = [
            {"role": "system", "content": enhanced_system_prompt},
            {"role": "user", "content": enhancement_user_prompt}
        ]
        new_system_content = call_api_with_retry(enhancement_messages, sample_id, 1024, "system_prompt", f"enhancement {i+1}")
        if new_system_content is None:
            return None
        
    # # Step 3: Slot extraction
    # slot_system_prompt = f"""HÃ£y Ä‘á»‹nh nghÄ©a cÃ¡c slot Ä‘á»ƒ Ä‘Ã¡nh dáº¥u cÃ¡c thÃ´ng tin bÃªn ngoÃ i mÃ  LLMs cáº§n Ä‘á»ƒ tuÃ¢n thá»§ System Prompt sau Ä‘Ã¢y:\n\"{new_system_content}\""""
    # slot_messages = [
    #     {"role": "system", "content": slot_system_prompt},
    #     {"role": "user", "content": slot_system_prompt}
    # ]
    # slot_content = call_api_with_retry(slot_messages, sample_id, 512, "slots", f"slot extraction {i+1}")
    # if slot_content is None:
    #     return None
    # # Step 4: Final system message
    # new_system_content = f"{new_system_content}\n\n{slot_content}"


    return {
        "id": sample_id,
        "messages": [{"role": "system", "content": new_system_content}]
    }

# Read input
with open(input_path, "r", encoding="utf-8") as f:
    dataset = [json.loads(line) for line in f]

# Process in parallel
with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor, open(output_path, "w", encoding="utf-8") as outfile:
    futures = {executor.submit(process_sample, idx, data): idx for idx, data in enumerate(dataset)}
    completed = 0

    # for future in as_completed(futures):
    #     result = future.result()
    #     if result:
    #         with output_lock:
    #             output_buffer.append(result)
    #             completed += 1

    #             if completed % SAVE_INTERVAL == 0:
    #                 for item in output_buffer:
    #                     outfile.write(json.dumps(item, ensure_ascii=False) + "\n")
    #                 outfile.flush()
    #                 output_buffer.clear()
    #                 print(f"âœ… Saved {completed} processed samples")

    with tqdm(total=len(futures), desc="Processing") as pbar:
        for future in as_completed(futures):
            result = future.result()
            if result:
                with output_lock:
                    output_buffer.append(result)
                    completed += 1

                    if completed % SAVE_INTERVAL == 0:
                        for item in output_buffer:
                            outfile.write(json.dumps(item, ensure_ascii=False) + "\n")
                        outfile.flush()
                        output_buffer.clear()
                        print(f"âœ… Saved {completed} processed samples")
            pbar.update(1)


    # Save remaining
    if output_buffer:
        for item in output_buffer:
            outfile.write(json.dumps(item, ensure_ascii=False) + "\n")
        print(f"âœ… Saved final {len(output_buffer)} samples")

print(f"\nğŸ‰ Done! Output written to: {output_path}")
